{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "current_path = Path.cwd()  \n",
    "project_root = current_path.parent \n",
    "sys.path.insert(0, str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.Data_Loader import data_loader\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder,RobustScaler,OrdinalEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_test, y_train, y_val, y_test= data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_val.shape, x_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"x_train\": x_train,\n",
    "    \"x_val\": x_val,\n",
    "    \"x_test\": x_test,\n",
    "    \"y_train\": y_train,\n",
    "    \"y_val\": y_val,\n",
    "    \"y_test\": y_test,\n",
    "}\n",
    "\n",
    "for name, ds in datasets.items():\n",
    "    has_nan = ds.isnull().any() if hasattr(ds, \"isnull\") else False\n",
    "    if hasattr(has_nan, \"any\"):\n",
    "        has_nan = has_nan.any()\n",
    "    if has_nan:\n",
    "        print(f\"missing values in {name}\")\n",
    "    else:\n",
    "        print(f\"no missing values in {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import ks_2samp\n",
    "# import pandas as pd\n",
    "\n",
    "# # Simple drift check (KS test) for numeric features\n",
    "\n",
    "# def drift_report(train_df, other_df, alpha=0.01):\n",
    "#     numeric_cols = train_df.select_dtypes(exclude=[\"object\", \"category\"]).columns\n",
    "#     rows = []\n",
    "#     for col in numeric_cols:\n",
    "#         train_col = train_df[col].dropna()\n",
    "#         other_col = other_df[col].dropna()\n",
    "#         if len(train_col) == 0 or len(other_col) == 0:\n",
    "#             rows.append((col, float(\"nan\"), False))\n",
    "#             continue\n",
    "#         stat, pval = ks_2samp(train_col, other_col)\n",
    "#         rows.append((col, pval, pval < alpha))\n",
    "#     report = pd.DataFrame(rows, columns=[\"feature\", \"p_value\", \"drift_flag\"])\n",
    "#     return report.sort_values(\"p_value\")\n",
    "\n",
    "# print(\"Drift vs val (alpha=0.01)\")\n",
    "# print(drift_report(x_train, x_val))\n",
    "# print(\"\\nDrift vs test (alpha=0.01)\")\n",
    "# print(drift_report(x_train, x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def build_preprocessor(X):\n",
    "    \"\"\"\n",
    "    Build preprocessing pipeline with separate handling for:\n",
    "    - Continuous numeric features (log + scale)\n",
    "    - Binary flags (no log, just scale)\n",
    "    - Ordinal features\n",
    "    - Categorical features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define original columns\n",
    "    yesNoColumns = [\"HasMortgage\", \"HasDependents\", \"HasCoSigner\"]\n",
    "    categorical_features = list(set(X.select_dtypes(include=['object'])) - set(yesNoColumns))\n",
    "    \n",
    "    # Original numeric features\n",
    "    original_numeric = [\n",
    "        'Age', 'Income', 'LoanAmount', 'CreditScore', \n",
    "        'MonthsEmployed', 'NumCreditLines', 'InterestRate', \n",
    "        'LoanTerm', 'DTIRatio'\n",
    "    ]\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # Continuous numeric transformer (with log)\n",
    "    continuous_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', RobustScaler())  # Better for outliers\n",
    "    ])\n",
    "    \n",
    "    # Binary transformer (NO log, just impute)\n",
    "\n",
    "    \n",
    "    # Yes/No columns\n",
    "    yesNoColumns_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('ordinal', OrdinalEncoder())\n",
    "    ])\n",
    "    \n",
    "    # Categorical\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    # Combine all\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('continuous', continuous_transformer, original_numeric ),\n",
    "            ('yesNo', yesNoColumns_transformer, yesNoColumns),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "preprocessor = build_preprocessor(x_train)\n",
    "x_train_processed = preprocessor.fit_transform(x_train)\n",
    "x_val_processed = preprocessor.transform(x_val)\n",
    "x_test_processed = preprocessor.transform(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_processed.shape, x_val_processed.shape, x_test_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neg = (y_train == 0).sum()\n",
    "num_pos = (y_train == 1).sum()\n",
    "scale_pos_weight_value = num_neg / num_pos\n",
    "scale_pos_weight_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try raw xgb\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    "    scale_pos_weight=scale_pos_weight_value,\n",
    "    random_state=42,\n",
    "    verbosity=0,\n",
    "\n",
    ")\n",
    "xgb_model.fit(x_train_processed, y_train, eval_set=[(x_val_processed, y_val)],verbose=False)\n",
    "\n",
    "# Evaluate on train ,test, val\n",
    "def evaluate_model(model, X, y, dataset_name=\"Dataset\"):\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "    auc = roc_auc_score(y, y_pred_proba)\n",
    "    print(f\"AUC on {dataset_name}: {auc:.4f}\")\n",
    "\n",
    "evaluate_model(xgb_model, x_train_processed, y_train, \"Train\")\n",
    "evaluate_model(xgb_model, x_val_processed, y_val, \"Validation\")\n",
    "evaluate_model(xgb_model, x_test_processed, y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "RocCurveDisplay.from_estimator(\n",
    "    xgb_model,\n",
    "    x_val_processed,\n",
    "    y_val,\n",
    "    name=\"XGBoost ROC Curve\"\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catboost\n",
    "from catboost import CatBoostClassifier\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    eval_metric='Logloss',\n",
    "    scale_pos_weight=scale_pos_weight_value,\n",
    "    random_seed=42,\n",
    "    \n",
    "    verbose=0\n",
    ")\n",
    "catboost_model.fit(x_train_processed, y_train , eval_set=(x_val_processed, y_val))\n",
    "evaluate_model(catboost_model, x_train_processed, y_train, \"Train\")\n",
    "evaluate_model(catboost_model, x_val_processed, y_val, \"Validation\")\n",
    "evaluate_model(catboost_model, x_test_processed, y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catboost classfication report\n",
    "from sklearn.metrics import classification_report\n",
    "y_val_pred = catboost_model.predict(x_val_processed)\n",
    "print(classification_report(y_val, y_val_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_estimator(\n",
    "    catboost_model,\n",
    "    x_val_processed,\n",
    "    y_val,\n",
    "    name=\"CatBoost ROC Curve\"\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm\n",
    "import lightgbm as lgb\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    metric='auc',\n",
    "    scale_pos_weight=scale_pos_weight_value,\n",
    "    random_state=42,\n",
    "    subsample_freq=1,\n",
    ")\n",
    "lgb_model.fit(x_train_processed, y_train,\n",
    "               eval_set=[(x_val_processed, y_val)],\n",
    "               callbacks=[#lgb.early_stopping(100), \n",
    "                          lgb.log_evaluation(period=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on train ,test, val\n",
    "evaluate_model(lgb_model, x_train_processed, y_train, \"Train\")\n",
    "evaluate_model(lgb_model, x_val_processed, y_val, \"Validation\")\n",
    "evaluate_model(lgb_model, x_test_processed, y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_estimator(\n",
    "    lgb_model,\n",
    "    x_val_processed,\n",
    "    y_val,\n",
    "    name=\"lgb ROC Curve\"\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# tunning xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import optuna\n",
    "import numpy as np\n",
    "import logging\n",
    "logging.getLogger(\"optuna\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class XgbTuner:\n",
    "    \"\"\"Hyperparameter tuning with proper AUC calculation and CV\"\"\"\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_val, y_val, scale_pos_weight, use_cv=True):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.scale_pos_weight = scale_pos_weight\n",
    "        self.use_cv = use_cv\n",
    "\n",
    "    def xgb_objective(self, trial):\n",
    "        params = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"auc\",\n",
    "            \"random_state\": 42,\n",
    "            \"scale_pos_weight\": self.scale_pos_weight,\n",
    "            \"device\":\"cuda\",\n",
    "            \"tree_method\":\"hist\",\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.1, 10.0, log=True),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 5.0)\n",
    "        }\n",
    "\n",
    "        if self.use_cv:\n",
    "            # Use cross-validation on training data\n",
    "            cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "            cv_scores = []\n",
    "            \n",
    "            for train_idx, val_idx in cv.split(self.X_train, self.y_train):\n",
    "                X_tr, X_vl = self.X_train[train_idx], self.X_train[val_idx]\n",
    "                y_tr, y_vl = self.y_train.iloc[train_idx], self.y_train.iloc[val_idx]\n",
    "                \n",
    "                model = xgb.XGBClassifier(**params, verbosity=0)\n",
    "                model.fit(X_tr, y_tr)\n",
    "                \n",
    "                probs = model.predict_proba(X_vl)[:, 1]\n",
    "                cv_scores.append(roc_auc_score(y_vl, probs))\n",
    "            \n",
    "            return float(np.mean(cv_scores))\n",
    "        else:\n",
    "            # Single validation set\n",
    "            model = xgb.XGBClassifier(**params, verbosity=0)\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            \n",
    "            # CRITICAL FIX: Use predict_proba\n",
    "            probs = model.predict_proba(self.X_val)[:, 1]\n",
    "            return float(roc_auc_score(self.y_val, probs))\n",
    "    \n",
    "    def tune_xgb(self, n_trials=100):\n",
    "        sampler = TPESampler(seed=42)\n",
    "        study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "        study.optimize(func=self.xgb_objective, n_trials=n_trials, show_progress_bar=True)\n",
    "        \n",
    "        print(f\"\\nBest AUC: {study.best_trial.value:.4f}\")\n",
    "        return study.best_trial.params, study.best_trial.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_instance=XgbTuner(X_train=x_train_processed,y_train=y_train,X_val=x_val_processed,y_val=y_val,scale_pos_weight=scale_pos_weight_value)\n",
    "best_params,best_score=tuner_instance.tune_xgb(n_trials=50)\n",
    "print('Best auc Score from tuning:',best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# tunning catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatBoostTuner:\n",
    "    \"\"\"Hyperparameter tuning for CatBoost with proper AUC calculation and CV\"\"\"\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_val, y_val, scale_pos_weight, use_cv=True):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.scale_pos_weight = scale_pos_weight\n",
    "        self.use_cv = use_cv\n",
    "\n",
    "    def catboost_objective(self, trial):\n",
    "        params = {\n",
    "            \"loss_function\": \"Logloss\",\n",
    "            \"eval_metric\": \"AUC\",\n",
    "            \"random_seed\": 42,\n",
    "            \"scale_pos_weight\": self.scale_pos_weight,\n",
    "            # \"task_type\": \"GPU\",\n",
    "            # \"devices\": \"0\",\n",
    "            \"verbose\": 0,\n",
    "            \"iterations\": trial.suggest_int(\"iterations\", 100, 1000),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-3, 10.0, log=True),\n",
    "            \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 1.0),\n",
    "            \"random_strength\": trial.suggest_float(\"random_strength\", 1e-3, 10.0, log=True),\n",
    "            \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n",
    "            \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 50),\n",
    "            \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"SymmetricTree\", \"Depthwise\", \"Lossguide\"]),\n",
    "        }\n",
    "        \n",
    "        # Add max_leaves only for Lossguide policy\n",
    "        if params[\"grow_policy\"] == \"Lossguide\":\n",
    "            params[\"max_leaves\"] = trial.suggest_int(\"max_leaves\", 16, 64)\n",
    "\n",
    "        if self.use_cv:\n",
    "            # Use cross-validation on training data\n",
    "            cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "            cv_scores = []\n",
    "            \n",
    "            for train_idx, val_idx in cv.split(self.X_train, self.y_train):\n",
    "                X_tr, X_vl = self.X_train[train_idx], self.X_train[val_idx]\n",
    "                y_tr, y_vl = self.y_train.iloc[train_idx], self.y_train.iloc[val_idx]\n",
    "                \n",
    "                model = CatBoostClassifier(**params)\n",
    "                model.fit(X_tr, y_tr, verbose=0)\n",
    "                \n",
    "                probs = model.predict_proba(X_vl)[:, 1]\n",
    "                cv_scores.append(roc_auc_score(y_vl, probs))\n",
    "            \n",
    "            return float(np.mean(cv_scores))\n",
    "        else:\n",
    "            # Single validation set\n",
    "            model = CatBoostClassifier(**params)\n",
    "            model.fit(self.X_train, self.y_train, verbose=0)\n",
    "            \n",
    "            probs = model.predict_proba(self.X_val)[:, 1]\n",
    "            return float(roc_auc_score(self.y_val, probs))\n",
    "    \n",
    "    def tune_catboost(self, n_trials=100):\n",
    "        sampler = TPESampler(seed=42)\n",
    "        study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "        study.optimize(func=self.catboost_objective, n_trials=n_trials, show_progress_bar=True)\n",
    "        \n",
    "        print(f\"\\nBest AUC: {study.best_trial.value:.4f}\")\n",
    "        return study.best_trial.params, study.best_trial.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_tuner = CatBoostTuner(\n",
    "    X_train=x_train_processed,\n",
    "    y_train=y_train,\n",
    "    X_val=x_val_processed,\n",
    "    y_val=y_val,\n",
    "    scale_pos_weight=scale_pos_weight_value\n",
    ")\n",
    "catboost_best_params, catboost_best_score = catboost_tuner.tune_catboost(n_trials=50)\n",
    "print('Best AUC Score from CatBoost tuning:', catboost_best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# tunning lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Convert to DataFrames with feature names (ensure dense array for type safety)\n",
    "x_train_processed = pd.DataFrame(np.asarray(x_train_processed), columns=feature_names, index=x_train.index)\n",
    "x_val_processed = pd.DataFrame(np.asarray(x_val_processed), columns=feature_names, index=x_val.index)\n",
    "x_test_processed = pd.DataFrame(np.asarray(x_test_processed), columns=feature_names, index=x_test.index)\n",
    "\n",
    "print(f\"Feature names ({len(feature_names)}):\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LGBMTuner:\n",
    "    \"\"\"Hyperparameter tuning for LightGBM with proper AUC calculation and CV\"\"\"\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_val, y_val, scale_pos_weight, use_cv=True):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.scale_pos_weight = scale_pos_weight\n",
    "        self.use_cv = use_cv\n",
    "\n",
    "    def lgbm_objective(self, trial):\n",
    "        params = {\n",
    "            \"objective\": \"binary\",\n",
    "            \"metric\": \"auc\",\n",
    "            \"random_state\": 42,\n",
    "            \"scale_pos_weight\": self.scale_pos_weight,\n",
    "            # \"device\": \"gpu\",\n",
    "            # \"gpu_platform_id\": 0,\n",
    "            # \"gpu_device_id\": 0,\n",
    "            \"verbosity\": -1,\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 150),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "            \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-3, 10.0, log=True),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-3, 10.0, log=True),\n",
    "            \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 1.0),\n",
    "            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        }\n",
    "\n",
    "        if self.use_cv:\n",
    "            # Use cross-validation on training data\n",
    "            cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "            cv_scores = []\n",
    "            \n",
    "            for train_idx, val_idx in cv.split(self.X_train, self.y_train):\n",
    "                X_tr, X_vl = self.X_train.iloc[train_idx], self.X_train.iloc[val_idx]\n",
    "                y_tr, y_vl = self.y_train.iloc[train_idx], self.y_train.iloc[val_idx]\n",
    "                \n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(X_tr, y_tr, callbacks=[lgb.log_evaluation(period=0)])\n",
    "                \n",
    "                probs = model.predict_proba(X_vl)[:, 1]\n",
    "                cv_scores.append(roc_auc_score(y_vl, probs))\n",
    "            \n",
    "            return float(np.mean(cv_scores))\n",
    "        else:\n",
    "            # Single validation set\n",
    "            model = lgb.LGBMClassifier(**params)\n",
    "            model.fit(self.X_train, self.y_train, callbacks=[lgb.log_evaluation(period=0)])\n",
    "            \n",
    "            probs = model.predict_proba(self.X_val)[:, 1]\n",
    "            return float(roc_auc_score(self.y_val, probs))\n",
    "    \n",
    "    def tune_lgbm(self, n_trials=100):\n",
    "        sampler = TPESampler(seed=42)\n",
    "        study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "        study.optimize(func=self.lgbm_objective, n_trials=n_trials, show_progress_bar=True)\n",
    "        \n",
    "        print(f\"\\nBest AUC: {study.best_trial.value:.4f}\")\n",
    "        return study.best_trial.params, study.best_trial.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_tuner = LGBMTuner(\n",
    "    X_train=x_train_processed,\n",
    "    y_train=y_train,\n",
    "    X_val=x_val_processed,\n",
    "    y_val=y_val,\n",
    "    scale_pos_weight=scale_pos_weight_value\n",
    ")\n",
    "lgbm_best_params, lgbm_best_score = lgbm_tuner.tune_lgbm(n_trials=25)\n",
    "print('Best AUC Score from LightGBM tuning:', lgbm_best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    metric='auc',\n",
    "    scale_pos_weight=scale_pos_weight_value,\n",
    "    random_state=42,\n",
    "    **lgbm_best_params\n",
    ")\n",
    "lgb_model.fit(x_train_processed, y_train,\n",
    "               eval_set=[(x_val_processed, y_val)],\n",
    "               callbacks=[#lgb.early_stopping(100), \n",
    "                          lgb.log_evaluation(period=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on train ,test, val\n",
    "evaluate_model(lgb_model, x_train_processed, y_train, \"Train\")\n",
    "evaluate_model(lgb_model, x_val_processed, y_val, \"Validation\")\n",
    "evaluate_model(lgb_model, x_test_processed, y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_estimator(\n",
    "    lgb_model,\n",
    "    x_val_processed,\n",
    "    y_val,\n",
    "    name=\"lgb ROC Curve\"\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "# now we will using lgbm pipline is faster and auc gap is small"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
